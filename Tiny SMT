import tarfile

from sklearn.model_selection import train_test_split

from collections import Counter, defaultdict
import random

with tarfile.open('de-en.tiny.tgz', 'r:gz') as tar:
 tar.extractall()

!ls

with open('de-en.tiny.de', 'r') as f:
  german = f.read().split('\n')[:-1]

with open('de-en.tiny.en', 'r') as f:
  english = f.read().split('\n')[:-1]

print("Данные языка X:\n", german)
print("Данные языка Y:\n", english)

X_train, X_test, y_train, y_test = train_test_split(english, german)

print("> Обучающая выборка:")
for text, label in zip(X_train, y_train):
    print(f"\nТекст на немецком: {label}\n Его перевод на английский: {text}\n")

print("> Тестовая выборка:")
for text, label in zip(X_test, y_test):
    print(f"\nТекст на немецком: {label}\n Его перевод на английский: {text}\n")

def tokenize(sentences):
  
  return [sentence.split() for sentence in sentences]


X_train_tokens, X_test_tokens, y_train_tokens, y_test_tokens = tokenize(X_train), tokenize(X_test), tokenize(y_train), tokenize(y_test)

print('Образец токенизированного текста:', X_train_tokens)

x_vocab = Counter(' '.join(german).split()).keys()
y_vocab = Counter(' '.join(english).split()).keys()

print(f"Словарь немецких словоформ: {x_vocab}\n Всего {len(x_vocab)} словоформ")
print(f"\nCловарь английских словоформ: {y_vocab}\n Всего {len(y_vocab)} словоформ")


uniform = 1 / (len(x_vocab) * len(y_vocab))

round(uniform, 3)

t = {}

for i in range(len(X_train)):
  
  for word_x in X_train_tokens[i]:
    for word_y in y_train_tokens[i]:
      
      t[(word_x, word_y)] = uniform


for elem in t:
  print("Соответствие |", elem[0], "  ->  ", elem[1], "| Вероятность:", round(t[elem], 3))

  epochs = 7
     

for epoch in range(epochs):
  

  
  count = {} 
  total = {} 

  for i in range(len(X_train)):
    
    for word_x in X_train_tokens[i]:
      for word_y in y_train_tokens[i]:
        
        count[(word_x, word_y)] = 0
        
        total[word_y] = 0

  
  for i in range(len(X_train)):
    
    total_stat = {} 

    
    for word_x in X_train_tokens[i]:
      total_stat[word_x] = 0 
      for word_y in y_train_tokens[i]:
        
        total_stat[word_x] += t[(word_x, word_y)]

    
    for word_x in X_train_tokens[i]:
      for word_y in y_train_tokens[i]:
        
        count[(word_x, word_y)] += t[(word_x, word_y)] / total_stat[word_x]
        
        total[word_y] += t[(word_x, word_y)] / total_stat[word_x]

  for i in range(len(X_train)):
    
    for word_x in X_train_tokens[i]:
      for word_y in y_train_tokens[i]:
        
        t[(word_x, word_y)] = count[(word_x, word_y)] / total[word_y]

for elem in t:
  print("Соответствие |", elem[0], "  ->  ", elem[1], "| Вероятность:", round(t[elem], 3))

  tokens = ' '.join(german).split()


bigram_model = defaultdict(list)


for i in range(len(tokens)-1):
    current_word = tokens[i]
    next_word = tokens[i + 1]
    bigram_model[current_word].append(next_word)

print(bigram_model)

def decoder(model, steps=5):
  current_word = random.choice(tokens)
  generated_sentence = current_word

  for step in range(steps):
    print('Шаг', step+1)
    next_word_options = model[current_word]
    print(f'Правдоподобные варианты продолжения для токена {current_word}:', next_word_options)

    current_word = random.choice(next_word_options)
    generated_sentence += ' '
    generated_sentence += current_word
    print('Промежуточный результат:', generated_sentence)
    print()
  print('Результат:', generated_sentence)

decoder(bigram_model)



sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)

def translate(token):
  for element in sorted_t:
    if element[0][1] == token:
      # поиск совпадений в t-table
      return element[0][0]

for sentence in y_test_tokens:
  print("Оригинальное предложение:", ' '.join(sentence))
  translation = []
  for token in sentence:
    translation.append(translate(token))
    print("Перевод:", ' '.join(translation))
  

  from nltk.translate.bleu_score import corpus_bleu

reference = [X_test_tokens[0], X_test_tokens[1]]
candidate = [translate(token) for token in y_test_tokens[0]]

bleu_score = corpus_bleu(reference, candidate)

print("BLEU Score:", bleu_score)

reference

candidate
